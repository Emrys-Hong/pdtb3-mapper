{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import codecs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename, fields_dict, index):\n",
    "    with open(filename) as f:\n",
    "        rawtext = f.readlines()\n",
    "    for relation in rawtext:\n",
    "        relation = relation.strip()\n",
    "        fields = relation.split('|')\n",
    "        fields = [str(filename).split('/')[-1]] + fields\n",
    "        fields_dict[index] = fields\n",
    "        index+=1\n",
    "    return fields_dict, index\n",
    "\n",
    "def get_filepaths(main_foldername):\n",
    "    filepaths = []\n",
    "    for folder in os.listdir(main_foldername):\n",
    "        for filename in os.listdir(main_foldername/folder):\n",
    "            filepaths.append(main_foldername/folder/filename)\n",
    "    return filepaths\n",
    "\n",
    "def generate_df(main_foldername='/home/pengfei/data/PDTB-3.0/data/gold/'):\n",
    "    main_foldername = Path(main_foldername)\n",
    "    filepaths = get_filepaths(main_foldername)\n",
    "    fields_dict = {}\n",
    "    cols = ['DocID','Relation_Type','Conn_SpanList','Conn_Src','Conn_Type','Conn_Pol','Conn_Det','Conn_Feat_SpanList','Conn1','SClass1A','SClass1B','Conn2','SClass2A','SClass2B','Sup1_SpanList','Arg1_SpanList','Arg1_Src','Arg1_Type','Arg1_Pol','Arg1_Det','Arg1_Feat_SpanList','Arg2_SpanList','Arg2_Src','Arg2_Type','Arg2_Pol','Arg2_Det','Arg2_Feat_SpanList','Sup2_SpanList','Adju_Reason','Adju_Disagr','PB_Role','PB_Verb','Offset','Provenance','Link']\n",
    "    index=0\n",
    "    for filepath in filepaths:\n",
    "        fields_dict, index = read_file(filepath, fields_dict, index)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(fields_dict, orient='index', columns=cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate csv file\n",
    "gold_folder = '../dataset/gold/'\n",
    "df = generate_df(gold_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53631, 35)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocID</th>\n",
       "      <th>Relation_Type</th>\n",
       "      <th>Conn_SpanList</th>\n",
       "      <th>Conn_Src</th>\n",
       "      <th>Conn_Type</th>\n",
       "      <th>Conn_Pol</th>\n",
       "      <th>Conn_Det</th>\n",
       "      <th>Conn_Feat_SpanList</th>\n",
       "      <th>Conn1</th>\n",
       "      <th>SClass1A</th>\n",
       "      <th>...</th>\n",
       "      <th>Arg2_Det</th>\n",
       "      <th>Arg2_Feat_SpanList</th>\n",
       "      <th>Sup2_SpanList</th>\n",
       "      <th>Adju_Reason</th>\n",
       "      <th>Adju_Disagr</th>\n",
       "      <th>PB_Role</th>\n",
       "      <th>PB_Verb</th>\n",
       "      <th>Offset</th>\n",
       "      <th>Provenance</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wsj_1222</td>\n",
       "      <td>Explicit</td>\n",
       "      <td>240..242</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>so</td>\n",
       "      <td>Contingency.Cause.Result</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>240..242</td>\n",
       "      <td>PDTB2::wsj_1222::240..242::SAME</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wsj_1222</td>\n",
       "      <td>Implicit</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>for instance</td>\n",
       "      <td>Expansion.Instantiation.Arg2-as-instance</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>413</td>\n",
       "      <td>PDTB2::wsj_1222::413::CHANGED</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wsj_1222</td>\n",
       "      <td>Implicit</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>specifically</td>\n",
       "      <td>Expansion.Level-of-detail.Arg2-as-detail</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>627</td>\n",
       "      <td>PDTB2::wsj_1222::627::SAME</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wsj_1222</td>\n",
       "      <td>EntRel</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>674</td>\n",
       "      <td>PDTB2::wsj_1222::674::SAME</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wsj_1222</td>\n",
       "      <td>Explicit</td>\n",
       "      <td>844..852;867..870</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>not only but</td>\n",
       "      <td>Expansion.Conjunction</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>844..870</td>\n",
       "      <td>PDTB3</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      DocID Relation_Type      Conn_SpanList Conn_Src Conn_Type Conn_Pol  \\\n",
       "0  wsj_1222      Explicit           240..242                               \n",
       "1  wsj_1222      Implicit                                                  \n",
       "2  wsj_1222      Implicit                                                  \n",
       "3  wsj_1222        EntRel                                                  \n",
       "4  wsj_1222      Explicit  844..852;867..870                               \n",
       "\n",
       "  Conn_Det Conn_Feat_SpanList         Conn1  \\\n",
       "0                                        so   \n",
       "1                              for instance   \n",
       "2                              specifically   \n",
       "3                                             \n",
       "4                              not only but   \n",
       "\n",
       "                                   SClass1A  ... Arg2_Det Arg2_Feat_SpanList  \\\n",
       "0                  Contingency.Cause.Result  ...                               \n",
       "1  Expansion.Instantiation.Arg2-as-instance  ...                               \n",
       "2  Expansion.Level-of-detail.Arg2-as-detail  ...                               \n",
       "3                                            ...                               \n",
       "4                     Expansion.Conjunction  ...                               \n",
       "\n",
       "  Sup2_SpanList Adju_Reason Adju_Disagr PB_Role PB_Verb    Offset  \\\n",
       "0                                                        240..242   \n",
       "1                                                             413   \n",
       "2                                                             627   \n",
       "3                                                             674   \n",
       "4                                                        844..870   \n",
       "\n",
       "                        Provenance Link  \n",
       "0  PDTB2::wsj_1222::240..242::SAME       \n",
       "1    PDTB2::wsj_1222::413::CHANGED       \n",
       "2       PDTB2::wsj_1222::627::SAME       \n",
       "3       PDTB2::wsj_1222::674::SAME       \n",
       "4                            PDTB3       \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span_list(span):\n",
    "    if span == '':\n",
    "        assert(False)\n",
    "    spans = span.split(';')\n",
    "    return [[int(k) for k in o.split('..')] for o in spans if o != '']\n",
    "\n",
    "\n",
    "def correct_conn_char_span(pdtb3, main_folder):\n",
    "    false_conn_list = []\n",
    "    for i in range(len(pdtb3)):\n",
    "        if pdtb3.loc[i, 'Relation_Type'] == 'Explicit':\n",
    "            spanlist = get_span_list(pdtb3.loc[i, 'Conn_SpanList'])\n",
    "            with open(main_folder + pdtb3.loc[i, 'DocID'], ) as f:\n",
    "                rawtext = f.read()\n",
    "                expected = ' '.join([rawtext[o[0]: o[1]] for o in spanlist]).lower()\n",
    "            if pdtb3.loc[i, 'Conn1'] not in expected:\n",
    "                print(expected, pdtb3.loc[i,'Conn1'])\n",
    "                false_conn_list.append(i)\n",
    "    print(\"Number of incorrect conn span list\", len(false_conn_list))\n",
    "#     print(\"begin correcting\")\n",
    "#     for i in false_conn_list:\n",
    "#         with open(main_folder+pdtb3.loc[i,'DocID'], ) as f:\n",
    "#             rawtext = f.read().lower()\n",
    "#             start = int(pdtb3.loc[i, 'Conn_SpanList'].split('..')[0])\n",
    "#             #TODO: dirty fix at 41854, \"still\", \"till\"\n",
    "#             distance = [abs(m.start() - start) for m in re.finditer(pdtb3.loc[i,'Conn1'], rawtext, )]\n",
    "#             start = [m.start() for m in re.finditer(pdtb3.loc[i, 'Conn1'], rawtext)]\n",
    "#             if distance != []:\n",
    "#                 index = distance.index(min(distance))\n",
    "#                 start = start[index]\n",
    "#                 span = str(start) + '..' + str(start+len(pdtb3.loc[i, 'Conn1']))\n",
    "#                 pdtb3.loc[i, 'Conn_SpanList'] = span\n",
    "#     print(\"connective char span are complete\")\n",
    "#     false_conn_list = []\n",
    "#     for i in range(len(pdtb3)):\n",
    "#         if pdtb3.loc[i, 'Relation_Type'] == 'Explicit':\n",
    "#             spanlist = get_span_list(pdtb3.loc[i, 'Conn_SpanList'])\n",
    "#             with open(main_folder + pdtb3.loc[i, 'DocID']) as f:\n",
    "#                 rawtext = f.read()\n",
    "#                 expected = ' '.join([rawtext[o[0]: o[1]] for o in spanlist]).lower()\n",
    "#             if pdtb3.loc[i, 'Conn1'] not in expected:\n",
    "#                 false_conn_list.append(i)\n",
    "#     print(\"Number of incorrect conn span left\", len(false_conn_list))\n",
    "#     return pdtb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "til till\n",
      "morever moreover\n",
      "morever moreover\n",
      "'cause because\n",
      "'til till\n",
      "Number of incorrect conn span list 5\n"
     ]
    }
   ],
   "source": [
    "correct_conn_char_span(df, '../dataset/raw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge_parse_dicts(folder):\n",
    "    folder = Path(folder)\n",
    "    filenames = os.listdir(folder)\n",
    "    ret = {}\n",
    "    for n in filenames:\n",
    "        ret.update(json.loads(codecs.open(folder/n, encoding='latin-1').read()))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_dict = load_and_merge_parse_dicts('../dataset/pdtb-parses/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_wrong_arg_span(parse_dict, pdtb3):\n",
    "    for i in range(len(pdtb3)):\n",
    "        char_start_list, char_end_list = get_list(parse_dict, pdtb3.loc[i, 'DocID'])\n",
    "        spanlist = get_span_list(pdtb3.loc[i, 'Arg1_SpanList'])\n",
    "        for span in spanlist:\n",
    "            if span[0] not in char_start_list:\n",
    "                distance = [abs(span[0]-o) for o in char_start_list]\n",
    "                index = distance.index(min(distance))\n",
    "                span[0] = char_start_list[index]\n",
    "                \n",
    "            if span[1] not in char_end_list:\n",
    "                distance = [abs(span[1]-o) for o in char_end_list]\n",
    "                index = distance.index(min(distance))\n",
    "                span[1] = char_end_list[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_all_files():\n",
    "    main_folder = '/home/pengfei/data/PDTB-3.0/data/raw/'\n",
    "    write_folder = '/home/pengfei/data/PDTB-3.0/all/raw/'\n",
    "    filepaths = [main_folder+folder+'/'+o for folder in os.listdir(main_folder) for o in os.listdir(main_folder+folder)]\n",
    "    for filename in filepaths:\n",
    "        with open(filename, encoding='latin1') as f:\n",
    "            rawtext = f.read()\n",
    "        with open(write_folder + filename.split('/')[-1], 'a') as f:\n",
    "            f.write(rawtext) \n",
    "    pass\n",
    "\n",
    "\n",
    "def correct_arg_span(parse_dict, pdtb3):\n",
    "    \"\"\"change are smaller than 2 char span\"\"\"\n",
    "    for i in range(len(pdtb3)):\n",
    "        if i%100==0:print(i)\n",
    "        if pdtb3.loc[i, 'DocID'] in parse_dict.keys():\n",
    "            char_start_list, char_end_list = get_list(parse_dict, pdtb3.loc[i, 'DocID'])\n",
    "\n",
    "            spanlist = get_span_list(pdtb3.loc[i, 'Arg1_SpanList'])\n",
    "            for span in spanlist:\n",
    "                if span[0] not in char_start_list:\n",
    "                    distance = [abs(span[0]-o) for o in char_start_list]\n",
    "                    index = distance.index(min(distance))\n",
    "                    span[0] = char_start_list[index]\n",
    "                if span[1] not in char_end_list:\n",
    "                    distance = [abs(span[1]-o) for o in char_end_list]\n",
    "                    index = distance.index(min(distance))\n",
    "                    span[1] = char_end_list[index]\n",
    "            span_string = get_span_string(spanlist)\n",
    "            pdtb3.loc[i, 'Arg1_SpanList'] = span_string\n",
    "\n",
    "            spanlist = get_span_list(pdtb3.loc[i, 'Arg2_SpanList'])\n",
    "            for span in spanlist:\n",
    "                if span[0] not in char_start_list:\n",
    "                    distance = [abs(span[0]-o) for o in char_start_list]\n",
    "                    index = distance.index(min(distance))\n",
    "                    span[0] = char_start_list[index]\n",
    "                if span[1] not in char_end_list:\n",
    "                    distance = [abs(span[1]-o) for o in char_end_list]\n",
    "                    index = distance.index(min(distance))\n",
    "                    span[1] = char_end_list[index]\n",
    "            span_string = get_span_string(spanlist)\n",
    "            pdtb3.loc[i, 'Arg2_SpanList'] = span_string\n",
    "\n",
    "    return pdtb3\n",
    "\n",
    "\n",
    "def merge3dicts(x, y, z):\n",
    "    m = x.copy()\n",
    "    m.update(y)\n",
    "    m.update(z)\n",
    "    return m\n",
    "\n",
    "\n",
    "def get_list(parse_dict, doc):\n",
    "    doc = parse_dict[doc]['sentences']\n",
    "    char_start_list = []\n",
    "    char_end_list = []\n",
    "    for sentence in doc:\n",
    "        for word in sentence['words']:\n",
    "            char_start_list.append(word[1]['CharacterOffsetBegin'])\n",
    "            char_end_list.append(word[1]['CharacterOffsetEnd'])\n",
    "    return char_start_list, char_end_list\n",
    "\n",
    "\n",
    "def get_span_string(span_list):\n",
    "    ret = ''\n",
    "    for span in span_list:\n",
    "        ret += str(span[0])\n",
    "        ret += '..'\n",
    "        ret += str(span[1])\n",
    "        ret += ';'\n",
    "    return ret[:-1]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gold_folder = '/home/pengfei/data/PDTB-3.0/data/gold/'\n",
    "    df = generate_df(gold_folder)\n",
    "    raw_folder = '/home/pengfei/data/PDTB-3.0/all/raw/'\n",
    "    df = correct_conn_char_span(df, raw_folder)\n",
    "    print(\"loading conll datasets\")\n",
    "    conll_train = '/home/pengfei/data/2015-2016_conll_shared_task/data/conll16st-en-03-29-16-train/pdtb-parses.json'\n",
    "    parse_dict_train = json.loads(codecs.open(conll_train, encoding='utf-8', errors='ignore').read())\n",
    "    conll_dev = '/home/pengfei/data/2015-2016_conll_shared_task/data/conll16st-en-03-29-16-dev/pdtb-parses.json'\n",
    "    parse_dict_dev = json.loads(codecs.open(conll_dev, encoding='utf-8', errors='ignore').read())\n",
    "    conll_test = '/home/pengfei/data/2015-2016_conll_shared_task/data/conll16st-en-03-29-16-test/pdtb-parses.json'\n",
    "    parse_dict_test = json.loads(codecs.open(conll_test, encoding='utf-8', errors='ignore').read())\n",
    "    print(\"datasets loaded\")\n",
    "    parse_dict = merge3dicts(parse_dict_train, parse_dict_dev, parse_dict_test)\n",
    "    df = correct_arg_span(parse_dict, df)\n",
    "    df.to_csv('pdtb3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
