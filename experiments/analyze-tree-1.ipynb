{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import Tree\n",
    "import re\n",
    "import json\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_newick_format(parse_tree):\n",
    "    parse_tree = parse_tree.replace(\",\", \"*COMMA*\")\n",
    "    parse_tree = parse_tree.replace(\":\", \"*COLON*\")\n",
    "    tree_list = load_syntax_tree(parse_tree)\n",
    "    if tree_list == None:\n",
    "        return None\n",
    "    tree_list = tree_list[1] #åŽ» root\n",
    "    s = syntax_tree_to_newick(tree_list)\n",
    "    s = s.replace(\",)\",\")\")\n",
    "    if s[-1] == \",\":\n",
    "        s = s[:-1] + \";\"\n",
    "    return s\n",
    "\n",
    "def load_syntax_tree(raw_text):\n",
    "    stack = [\"ROOT\"]\n",
    "    text = re.sub(r\"\\(\", \" ( \", raw_text)\n",
    "    text = re.sub(r\"\\)\", \" ) \", text)\n",
    "    text = re.sub(r\"\\n\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"^\\(\\s*\\(\\s*\", \"\", text)\n",
    "    text = re.sub(r\"\\s*\\)\\s*\\)$\", \"\", text)\n",
    "    for c in text.strip(\" \").split(\" \"):\n",
    "        if c == \")\":\n",
    "            node = []\n",
    "            while(1):\n",
    "                popped = stack.pop()\n",
    "                if popped == \"(\":\n",
    "                    break\n",
    "                node.append(popped)\n",
    "            node.reverse()\n",
    "            if len(node) > 1:\n",
    "                stack.append(node)\n",
    "            else:\n",
    "                if node == []:\n",
    "                    return None\n",
    "                stack.append(node[0])\n",
    "        else:\n",
    "            stack.append(c)\n",
    "    return stack\n",
    "\n",
    "def syntax_tree_to_newick(syntax_tree):\n",
    "    s = \"(\"\n",
    "    for child in syntax_tree[1:]:\n",
    "        if not isinstance(child,list):\n",
    "            s += child\n",
    "        else:\n",
    "            s += syntax_tree_to_newick(child)\n",
    "    s += \")\" + str(syntax_tree[0]) + \",\"\n",
    "    return s\n",
    "\n",
    "\n",
    "def get_all_tree(parse_tree):\n",
    "    parse_tree_text = to_newick_format(parse_tree)\n",
    "    tree = Tree(parse_tree_text, format=1)\n",
    "    treelist = []\n",
    "    tree_dict = {o:str(i) for i,o in enumerate(tree.get_leaves())}\n",
    "    return [[int(i) for i in o.split()] for o in set(_get_all_tree(tree, treelist, tree_dict))]\n",
    "\n",
    "def _get_all_tree(tree, treelist, tree_dict):\n",
    "    punct = ['.', ',']\n",
    "    treelist.append(' '.join([tree_dict[o] for o in tree.get_leaves() if str(o).split('-')[-1] not in punct]))\n",
    "    if tree.get_children() == []:\n",
    "        return treelist\n",
    "    else:\n",
    "        for child in tree.get_children():\n",
    "            treelist = _get_all_tree(child, treelist, tree_dict)\n",
    "        return treelist\n",
    "    \n",
    "\n",
    "def merge3dicts(x, y, z):\n",
    "    m = x\n",
    "    m.update(y)\n",
    "    m.update(z)\n",
    "    return m\n",
    "\n",
    "def get_related_doc(parse_data, docid):\n",
    "    ret = []\n",
    "    for i, r in enumerate(parse_data):\n",
    "        if r['DocID'] == docid:\n",
    "            ret.append(r)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_tree = \"( (S (S (NP (DT Some)) (VP (MD may) (VP (VB have) (VP (VBN forgotten))))) (: --) (CC and) (S (NP (DT some) (JJR younger) (NNS ones)) (VP (MD may) (ADVP (RB never)) (VP (VB have) (ADJP (JJ experienced)) (: --) (SBAR (WHNP (WP what)) (S (NP (PRP it)) (VP (VBZ 's) (VP (VB like) (S (VP (TO to) (VP (VB invest) (PP (IN during) (NP (DT a) (NN recession))))))))))))) (. .)) )\"\n",
    "# all_tree = get_all_tree(parse_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets loaded\n"
     ]
    }
   ],
   "source": [
    "conll_train = '/home/pengfei/data/2015-2016_conll_shared_task/data/conll16st-en-03-29-16-train/pdtb-parses.json'\n",
    "parse_dict_train = json.loads(codecs.open(conll_train, encoding='utf-8', errors='ignore').read())\n",
    "conll_dev = '/home/pengfei/data/2015-2016_conll_shared_task/data/conll16st-en-03-29-16-dev/pdtb-parses.json'\n",
    "parse_dict_dev = json.loads(codecs.open(conll_dev, encoding='utf-8', errors='ignore').read())\n",
    "conll_test = '/home/pengfei/data/2015-2016_conll_shared_task/data/conll16st-en-03-29-16-test/pdtb-parses.json'\n",
    "parse_dict_test = json.loads(codecs.open(conll_test, encoding='utf-8', errors='ignore').read())\n",
    "parse_dict = merge3dicts(parse_dict_train, parse_dict_dev, parse_dict_test)\n",
    "\n",
    "parse_data_path = \"/home/pengfei/data/2015-2016_conll_shared_task/data/conll16st-en-03-29-16-train/relations.json\"\n",
    "parse_data_dev_path = '/home/pengfei/data/2015-2016_conll_shared_task/data/conll16st-en-03-29-16-dev/relations.json'\n",
    "parse_data_test_path = '/home/pengfei/data/2015-2016_conll_shared_task/data/conll16st-en-03-29-16-test/relations.json'\n",
    "parse_data = [json.loads(line) for line in codecs.open(parse_data_path).readlines()]\n",
    "parse_data_dev = [json.loads(line) for line in codecs.open(parse_data_dev_path).readlines()]\n",
    "parse_data_test = [json.loads(line) for line in codecs.open(parse_data_test_path).readlines()]\n",
    "parse_data.extend(parse_data_dev)\n",
    "parse_data.extend(parse_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "true = 0\n",
    "false = 0\n",
    "k=0\n",
    "for DocID in list(parse_dict.keys()):\n",
    "    k=0\n",
    "    related_doc = get_related_doc(parse_data, DocID)\n",
    "    doc_trees = {}\n",
    "    for i, sentence in enumerate(parse_dict[DocID]['sentences']):\n",
    "        try:\n",
    "            doc_trees[i] = get_all_tree(sentence['parsetree'])\n",
    "        except Exception:\n",
    "            k = 1\n",
    "    if k == 1:\n",
    "        continue\n",
    "    for relation in related_doc:\n",
    "        arg1_sentence_id = [o[3] for o in relation['Arg1']['TokenList']]\n",
    "        if len(set(arg1_sentence_id)) == 1:\n",
    "            arg1_token = [o[4] for o in relation['Arg1']['TokenList']]\n",
    "\n",
    "        if arg1_token in doc_trees[arg1_sentence_id[0]]:\n",
    "#             print(True)\n",
    "            true += 1\n",
    "        else:\n",
    "            false += 1\n",
    "#             print(' '.join([parse_dict[DocID]['sentences'][arg1_sentence_id[0]]['words'][o][0] for o in arg1_token]))\n",
    "#             print()\n",
    "#             for tree_index in doc_trees[arg1_sentence_id[0]]:\n",
    "#                 print(' '.join([parse_dict[DocID]['sentences'][arg1_sentence_id[0]]['words'][o][0] for o in tree_index]))\n",
    "#             print(False)\n",
    "#             print('========================================')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"( (S (NP (DT The) (NN following)) (VP (VBD were) (PP (IN among) (NP (NP (NP (NNP Friday) (POS 's)) (NNS offerings) (CC and) (NNS pricings)) (PP (IN in) (NP (DT the) (UCP (NNP U.S.) (CC and) (JJ non-U.S.)) (NN capital) (NNS markets))))) (, ,) (PP (IN with) (NP (NP (NNS terms)) (CC and) (NP (NN syndicate) (NN manager)))) (, ,) (SBAR (IN as) (S (VP (VBN compiled) (PP (IN by) (NP (NNP Dow) (NNP Jones) (NNP Capital) (NNP Markets) (NNP Report))))))) (: :)) )\\n\""
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_dict[DocID]['sentences'][0]['parsetree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6426481620405101"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true / (true + false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets loaded\n"
     ]
    }
   ],
   "source": [
    "conll_train = '/home/pengfei/data/PDTB-3.0/all/conll/train/pdtb-parses.json'\n",
    "parse_dict_train = json.loads(codecs.open(conll_train, encoding='utf-8', errors='ignore').read())\n",
    "conll_dev = '/home/pengfei/data/PDTB-3.0/all/conll/dev/pdtb-parses.json'\n",
    "parse_dict_dev = json.loads(codecs.open(conll_dev, encoding='utf-8', errors='ignore').read())\n",
    "conll_test = '/home/pengfei/data/PDTB-3.0/all/conll/test/pdtb-parses.json'\n",
    "parse_dict_test = json.loads(codecs.open(conll_test, encoding='utf-8', errors='ignore').read())\n",
    "print(\"datasets loaded\")\n",
    "parse_dict = merge3dicts(parse_dict_train, parse_dict_dev, parse_dict_test)\n",
    "\n",
    "parse_data_path = \"/home/pengfei/data/PDTB-3.0/all/conll/train/relations.json\"\n",
    "parse_data_dev_path = '/home/pengfei/data/PDTB-3.0/all/conll/dev/relations.json'\n",
    "parse_data_test_path = '/home/pengfei/data/PDTB-3.0/all/conll/test/relations.json'\n",
    "parse_data = [json.loads(line) for line in codecs.open(parse_data_path).readlines()]\n",
    "parse_data_dev = [json.loads(line) for line in codecs.open(parse_data_dev_path).readlines()]\n",
    "parse_data_test = [json.loads(line) for line in codecs.open(parse_data_test_path).readlines()]\n",
    "parse_data.extend(parse_data_dev)\n",
    "parse_data.extend(parse_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = 0\n",
    "false = 0\n",
    "k=0\n",
    "for DocID in list(parse_dict.keys()):\n",
    "    k=0\n",
    "    related_doc = get_related_doc(parse_data, DocID)\n",
    "    doc_trees = {}\n",
    "    for i, sentence in enumerate(parse_dict[DocID]['sentences']):\n",
    "        try:\n",
    "            doc_trees[i] = get_all_tree(sentence['parsetree'])\n",
    "        except Exception:\n",
    "            k = 1\n",
    "    if k == 1:\n",
    "        continue\n",
    "    for relation in related_doc:\n",
    "        arg1_sentence_id = [o[3] for o in relation['Arg1']['TokenList']]\n",
    "        if len(set(arg1_sentence_id)) == 1:\n",
    "            arg1_token = [o[4] for o in relation['Arg1']['TokenList']]\n",
    "\n",
    "        if arg1_token in doc_trees[arg1_sentence_id[0]]:\n",
    "#             print(True)\n",
    "            true += 1\n",
    "        else:\n",
    "            false += 1\n",
    "#             print(' '.join([parse_dict[DocID]['sentences'][arg1_sentence_id[0]]['words'][o][0] for o in arg1_token]))\n",
    "#             print()\n",
    "#             for tree_index in doc_trees[arg1_sentence_id[0]]:\n",
    "#                 print(' '.join([parse_dict[DocID]['sentences'][arg1_sentence_id[0]]['words'][o][0] for o in tree_index]))\n",
    "#             print(False)\n",
    "#             print('========================================')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6426481620405101"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true / (true + false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
