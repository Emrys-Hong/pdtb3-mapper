{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from api.api import PDTB\n",
    "from constituent import Constituent\n",
    "import itertools\n",
    "import copy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdtb2 = '/home/pengfei/data/pdtb_v2/all/conll/'\n",
    "pdtb3 = PDTB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## only for one individual sentence for explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_constituents(docid, sentid, conn_index):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "            docid(str)\n",
    "            sentid(int)\n",
    "            conn_index(list[int])\n",
    "    Returns:\n",
    "            [ [(token_index, text),...], ...]\"\"\"\n",
    "    syntax_tree = pdtb3.get_syntax_tree(docid, sentid)\n",
    "    if syntax_tree.tree == None:\n",
    "        return []\n",
    "    all_leaves = syntax_tree.tree.get_leaves()\n",
    "    conn_indices = conn_index\n",
    "    constituent_nodes = []\n",
    "    if len(conn_indices) == 1:# like and or so...\n",
    "        conn_node = syntax_tree.get_leaf_node_by_token_index(conn_indices[0]).up\n",
    "    else:\n",
    "        conn_node = syntax_tree.get_common_ancestor_by_token_indices(conn_indices)\n",
    "        conn_leaves = set([all_leaves.index(syntax_tree.get_leaf_node_by_token_index(conn_index)) for conn_index in conn_indices])\n",
    "        children = conn_node.get_children()\n",
    "        for child in children:\n",
    "            leaves = set([all_leaves.index(n) for n in child.get_leaves()])\n",
    "            if list(leaves-conn_leaves)!=[]: constituent_nodes.append(list(leaves-conn_leaves))\n",
    "    curr = conn_node\n",
    "    while not curr.is_root():\n",
    "        sibs = [ [all_leaves.index(n) for n in sib] for sib in syntax_tree.get_siblings(curr)]\n",
    "        constituent_nodes.extend(sibs)\n",
    "        curr = curr.up\n",
    "\n",
    "    tokens_indices_with_text = pdtb3.get_tokens_indices_with_text(docid, sentid)\n",
    "    subtree_list = [ [(o,tokens_indices_with_text[o][2]) for o in node] for node in constituent_nodes]\n",
    "    return subtree_list\n",
    "\n",
    "def merge_consti(subtree_list):\n",
    "    return {(o[0][0],o[-1][0]):i for i,o in enumerate(subtree_list)}\n",
    "\n",
    "def arg_is_contained(i, arg):\n",
    "    docid = pdtb3.parse_data[i]['DocID']\n",
    "    sentid = pdtb3.get_arg_sent_id(i, arg)\n",
    "    conn_indices = [o[1] for o in pdtb3.get_arg_token_list(i, 'Connective')]\n",
    "    subtree_list = get_constituents(docid, sentid[0], conn_indices)\n",
    "    merged_consti_list = merge_consti(subtree_list)\n",
    "    token_list = pdtb3.get_arg_token_list(i, arg)\n",
    "    start,end = token_list[0][1], token_list[-1][1]\n",
    "    # filter results\n",
    "    consti_list = [k for k in merged_consti_list.keys() if k[0]>=start and k[1]<=end]\n",
    "    # generate results\n",
    "    results = []\n",
    "    for i in range(1, len(consti_list)+1):\n",
    "        for subset in itertools.combinations(consti_list, i):\n",
    "            results.append(expand(subset))\n",
    "    # check\n",
    "    return [o[1] for o in token_list] in results\n",
    "\n",
    "def expand(subset):\n",
    "    ret = []\n",
    "    for subsubset in subset:\n",
    "        for i in range(subsubset[0], subsubset[1]+1):\n",
    "            ret.append(i)\n",
    "    return sorted(ret)\n",
    "\n",
    "def get_subtree_length(i, arg):\n",
    "    docid = pdtb3.parse_data[i]['DocID']\n",
    "    sentid = pdtb3.get_arg_sent_id(i, arg)\n",
    "    conn_indices = [o[1] for o in pdtb3.get_arg_token_list(i, 'Connective')]\n",
    "    subtree_list = get_constituents(docid, sentid[0], conn_indices)\n",
    "    return len(subtree_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arg1 cover rate  0.8715415019762845\n",
      "arg2 cover rate  0.9189723320158103\n"
     ]
    }
   ],
   "source": [
    "bad = 0\n",
    "total =0\n",
    "for i in range(len(pdtb3.parse_data_test)):\n",
    "    if pdtb3.parse_data[i]['Type'] == 'Explicit' and pdtb3.get_arg_sent_id(i,'Arg1')[0] == pdtb3.get_arg_sent_id(i,'Arg2')[0]:\n",
    "        total+=1\n",
    "        if not arg_is_contained(i, 'Arg1'):\n",
    "            bad += 1\n",
    "#             print(pdtb3.get_highlighted_parsetree(i, v=True))\n",
    "print('arg1 cover rate ', 1- bad/total)\n",
    "\n",
    "\n",
    "bad = 0\n",
    "total =0\n",
    "for i in range(len(pdtb3.parse_data_test)):\n",
    "    if pdtb3.parse_data[i]['Type'] == 'Explicit' and pdtb3.get_arg_sent_id(i,'Arg1')[0] == pdtb3.get_arg_sent_id(i,'Arg2')[0]:\n",
    "        total+=1\n",
    "        if not arg_is_contained(i, 'Arg2'):\n",
    "            bad += 1\n",
    "#             print(pdtb3.get_highlighted_parsetree(i, v=True))\n",
    "print('arg2 cover rate ', 1- bad/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arg1 7.988142292490118\n",
      "arg2 7.988142292490118\n"
     ]
    }
   ],
   "source": [
    "length = 0\n",
    "total =0\n",
    "for i in range(len(pdtb3.parse_data_test)):\n",
    "    if pdtb3.parse_data[i]['Type'] == 'Explicit' and pdtb3.get_arg_sent_id(i,'Arg1')[0] == pdtb3.get_arg_sent_id(i,'Arg2')[0]:\n",
    "        total+=1\n",
    "        length += get_subtree_length(i, 'Arg1')\n",
    "print('arg1', length/total)\n",
    "\n",
    "\n",
    "length = 0\n",
    "total =0\n",
    "for i in range(len(pdtb3.parse_data_test)):\n",
    "    if pdtb3.parse_data[i]['Type'] == 'Explicit' and pdtb3.get_arg_sent_id(i,'Arg1')[0] == pdtb3.get_arg_sent_id(i,'Arg2')[0]:\n",
    "        total+=1\n",
    "        length += get_subtree_length(i, 'Arg2')\n",
    "print('arg2', length/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def arg_clauses(docid, sentid):\n",
    "#     token_indices_with_text = pdtb3.get_tokens_indices_with_text(docid, sentid)\n",
    "#     sent_tokens = [(i,t) for _,i,t in token_indices_with_text]\n",
    "#     # step 1: split the word\n",
    "#     punctuation = \"...,:;?!~--\"\n",
    "#     _clause_indices_list = []#[[(1,\"I\")..], ..]\n",
    "#     temp = []\n",
    "#     for index, word in sent_tokens:\n",
    "#         if word not in punctuation:\n",
    "#             temp.append((index, word))\n",
    "#         else:\n",
    "#             if temp != []:\n",
    "#                 _clause_indices_list.append(temp)\n",
    "#                 temp = []\n",
    "#     if temp != []: _clause_indices_list.append(temp)\n",
    "#     # strip punctuations in the start or end \n",
    "#     clause_indices_list = []\n",
    "#     for clause_indices in _clause_indices_list:\n",
    "#         temp = list_strip_punctuation(clause_indices)\n",
    "#         if temp != []:\n",
    "#             clause_indices_list.append([item[0] for item in temp])\n",
    "\n",
    "#     # step2: then use SBAR tag in its parse tree to split each part into clauses.\n",
    "#     syntax_tree = pdtb3.get_syntax_tree(docid, sentid)\n",
    "#     if syntax_tree.tree == None:\n",
    "#         return []\n",
    "#     clause_list = []\n",
    "#     for clause_indices in clause_indices_list:\n",
    "#         clause_tree = _get_subtree(syntax_tree, clause_indices)\n",
    "#         flag = 0\n",
    "#         for node in clause_tree.tree.traverse(strategy=\"levelorder\"):\n",
    "#             if node.name == \"SBAR\":\n",
    "#                 temp1 = [node.index for node in node.get_leaves()]\n",
    "#                 temp2 = sorted(list(set(clause_indices) - set(temp1)))\n",
    "\n",
    "#                 if temp2 == []:\n",
    "#                     clause_list.append(temp1)\n",
    "#                 else:\n",
    "#                     if temp1[0] < temp2 [0]:\n",
    "#                         clause_list.append(temp1)\n",
    "#                         clause_list.append(temp2)\n",
    "#                     else:\n",
    "#                         clause_list.append(temp2)\n",
    "#                         clause_list.append(temp1)\n",
    "#                 flag = 1\n",
    "#                 break\n",
    "#         if flag == 0:\n",
    "#             clause_list.append(clause_indices)\n",
    "#     clauses = []\n",
    "#     for clause_indices in clause_list:\n",
    "#         clauses.append(clause_indices)\n",
    "#     return [ [(o,token_indices_with_text[o][2]) for o in c] for c in clauses]\n",
    "\n",
    "# def list_strip_punctuation(list):\n",
    "#     punctuation = \"\"\"!\"#&'*+,-..../:;<=>?@[\\]^_`|~\"\"\" + \"``\" + \"''\"\n",
    "#     i = 0\n",
    "#     while i < len(list) and list[i][1] in punctuation + \"-LCB--LRB-\":\n",
    "#         i += 1\n",
    "#     if i == len(list):\n",
    "#         return []\n",
    "#     j = len(list) - 1\n",
    "#     while j >= 0 and list[j][1] in punctuation + \"-RRB--RCB-\":\n",
    "#         j -= 1\n",
    "#     return list[i: j+1]\n",
    "\n",
    "# def _get_subtree(syntax_tree, clause_indices):\n",
    "#     copy_tree = copy.deepcopy(syntax_tree)\n",
    "\n",
    "#     for index, leaf in enumerate(copy_tree.tree.get_leaves()):\n",
    "#         leaf.add_feature(\"index\",index)\n",
    "\n",
    "#     clause_nodes = []\n",
    "#     for index in clause_indices:\n",
    "#         node = copy_tree.get_leaf_node_by_token_index(index)\n",
    "#         clause_nodes.append(node)\n",
    "\n",
    "#     for node in copy_tree.tree.traverse(strategy=\"levelorder\"):\n",
    "#         node_leaves = node.get_leaves()\n",
    "#         if set(node_leaves) & set(clause_nodes) == set([]):\n",
    "#             node.detach()\n",
    "#     return copy_tree\n",
    "\n",
    "\n",
    "# def merge_consti(subtree_list):\n",
    "#     return {(o[0][0],o[-1][0]):i for i,o in enumerate(subtree_list)}\n",
    "\n",
    "# def expand(subset):\n",
    "#     ret = []\n",
    "#     for subsubset in subset:\n",
    "#         for i in range(subsubset[0], subsubset[1]+1):\n",
    "#             ret.append(i)\n",
    "#     return sorted(ret)\n",
    "\n",
    "# def get_subtree_length(i, arg):\n",
    "#     docid = pdtb3.parse_data[i]['DocID']\n",
    "#     sentid = pdtb3.get_arg_sent_id(i, arg)\n",
    "#     conn_indices = [o[1] for o in pdtb3.get_arg_token_list(i, 'Connective')]\n",
    "#     subtree_list = get_constituents(docid, sentid[0], conn_indices)\n",
    "#     return len(subtree_list)\n",
    "\n",
    "\n",
    "# def arg_is_contained_clause(i, arg):\n",
    "#     docid = pdtb3.parse_data[i]['DocID']\n",
    "#     sentids = pdtb3.get_arg_sent_id(i, arg)\n",
    "#     clause_list = arg_clauses(docid, sentid[0])\n",
    "#     merged_clause_list = merge_consti(clause_list)\n",
    "#     token_list = pdtb3.get_arg_token_list(i, arg)\n",
    "#     token_text = pdtb3.get_tokens_text(docid, token_list)\n",
    "#     assert(len(token_list) == len(token_text))\n",
    "#     start,end = token_list[0][1], token_list[-1][1]\n",
    "#     # filter results\n",
    "#     clause_list = [k for k in merged_clause_list.keys() if k[0]>=start and k[1]<=end]\n",
    "#     # generate results\n",
    "#     results = []\n",
    "#     for i in range(1, len(clause_list)+1):\n",
    "#         for subset in itertools.combinations(clause_list, i):\n",
    "#             results.append(expand(subset))\n",
    "#     # check\n",
    "#     return contained([o[1] for o in token_list], results, token_text)\n",
    "\n",
    "# def contained(token_list, results, token_text):\n",
    "#     punctuation = \"\"\"!\"#&'*+,-..../:;<=>?@[\\]^_`|~\"\"\" + \"``\" + \"''\"\n",
    "#     for result in results:\n",
    "#         remain = set(token_list).symmetric_difference(set(result))\n",
    "#         whether = True\n",
    "#         for r in remain:\n",
    "#             if r not in token_list: continue\n",
    "#             if token_text[token_list.index(r)] not in punctuation:\n",
    "#                 whether = False\n",
    "#         if whether: return True\n",
    "#     return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# only for one individual sentence for implicit\n",
    "# for PS case for explicit\n",
    "# for PS case for implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{False: 0.12113295751409323, True: 0.8788670424859067}\n",
      "{False: 0.0881, True: 0.6392}\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "total = 10000\n",
    "for i in range(total):\n",
    "    if not (pdtb3.parse_data[i]['Type'] == 'Explicit' and pdtb3.get_arg_sent_id(i,'Arg1')[0] == pdtb3.get_arg_sent_id(i,'Arg2')[0]):\n",
    "        a.append(pdtb3.arg_is_contained_clause(i, 'Arg1'))\n",
    "dis = Counter(a)\n",
    "print({k:v/len(a) for k,v in dis.items()})\n",
    "print({k:v/total for k,v in dis.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{True: 0.8222191667812457, False: 0.1777808332187543}\n",
      "{True: 0.598, False: 0.1293}\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "total = 10000\n",
    "for i in range(total):\n",
    "    if not (pdtb3.parse_data[i]['Type'] == 'Explicit' and pdtb3.get_arg_sent_id(i,'Arg1')[0] == pdtb3.get_arg_sent_id(i,'Arg2')[0]):\n",
    "        a.append(pdtb3.arg_is_contained_clause(i, 'Arg2'))\n",
    "dis = Counter(a)\n",
    "print({k:v/len(a) for k,v in dis.items()})\n",
    "print({k:v/total for k,v in dis.items()})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
