{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import Tree\n",
    "import re\n",
    "import json\n",
    "import codecs\n",
    "from syntax_tree import Syntax_tree\n",
    "from constituent import Constituent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_newick_format(parse_tree):\n",
    "    parse_tree = parse_tree.replace(\",\", \"*COMMA*\")\n",
    "    parse_tree = parse_tree.replace(\":\", \"*COLON*\")\n",
    "    tree_list = load_syntax_tree(parse_tree)\n",
    "    if tree_list == None:\n",
    "        return None\n",
    "    tree_list = tree_list[1] #åŽ» root\n",
    "    s = syntax_tree_to_newick(tree_list)\n",
    "    s = s.replace(\",)\",\")\")\n",
    "    if s[-1] == \",\":\n",
    "        s = s[:-1] + \";\"\n",
    "    return s\n",
    "\n",
    "def load_syntax_tree(raw_text):\n",
    "    stack = [\"ROOT\"]\n",
    "    text = re.sub(r\"\\(\", \" ( \", raw_text)\n",
    "    text = re.sub(r\"\\)\", \" ) \", text)\n",
    "    text = re.sub(r\"\\n\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"^\\(\\s*\\(\\s*\", \"\", text)\n",
    "    text = re.sub(r\"\\s*\\)\\s*\\)$\", \"\", text)\n",
    "    for c in text.strip(\" \").split(\" \"):\n",
    "        if c == \")\":\n",
    "            node = []\n",
    "            while(1):\n",
    "                popped = stack.pop()\n",
    "                if popped == \"(\":\n",
    "                    break\n",
    "                node.append(popped)\n",
    "            node.reverse()\n",
    "            if len(node) > 1:\n",
    "                stack.append(node)\n",
    "            else:\n",
    "                if node == []:\n",
    "                    return None\n",
    "                stack.append(node[0])\n",
    "        else:\n",
    "            stack.append(c)\n",
    "    return stack\n",
    "\n",
    "def syntax_tree_to_newick(syntax_tree):\n",
    "    s = \"(\"\n",
    "    for child in syntax_tree[1:]:\n",
    "        if not isinstance(child,list):\n",
    "            s += child\n",
    "        else:\n",
    "            s += syntax_tree_to_newick(child)\n",
    "    s += \")\" + str(syntax_tree[0]) + \",\"\n",
    "    return s\n",
    "\n",
    "\n",
    "def get_all_tree(parse_tree):\n",
    "    parse_tree_text = to_newick_format(parse_tree)\n",
    "    tree = Tree(parse_tree_text, format=1)\n",
    "    treelist = []\n",
    "    tree_dict = {o:str(i) for i,o in enumerate(tree.get_leaves())}\n",
    "    return [[int(i) for i in o.split()] for o in set(_get_all_tree(tree, treelist, tree_dict))]\n",
    "\n",
    "def _get_all_tree(tree, treelist, tree_dict):\n",
    "    punct = ['.', ',']\n",
    "    treelist.append(' '.join([tree_dict[o] for o in tree.get_leaves() if str(o).split('-')[-1] not in punct]))\n",
    "    if tree.get_children() == []:\n",
    "        return treelist\n",
    "    else:\n",
    "        for child in tree.get_children():\n",
    "            treelist = _get_all_tree(child, treelist, tree_dict)\n",
    "        return treelist\n",
    "    \n",
    "\n",
    "def merge3dicts(x, y, z):\n",
    "    m = x\n",
    "    m.update(y)\n",
    "    m.update(z)\n",
    "    return m\n",
    "\n",
    "def get_related_doc(parse_data, docid):\n",
    "    ret = []\n",
    "    for i, r in enumerate(parse_data):\n",
    "        if r['DocID'] == docid:\n",
    "            ret.append(r)\n",
    "    return ret\n",
    "\n",
    "def _get_constituents(parse_dict, DocID, sent_index, conn_index):\n",
    "    parse_tree = parse_dict[DocID][\"sentences\"][sent_index][\"parsetree\"].strip()\n",
    "    syntax_tree = Syntax_tree(parse_tree)\n",
    "    if syntax_tree.tree == None:\n",
    "        return []\n",
    "    conn_indices = conn_index\n",
    "    constituent_nodes = []\n",
    "    if len(conn_indices) == 1:# like and or so...\n",
    "        conn_node = syntax_tree.get_leaf_node_by_token_index(conn_indices[0]).up\n",
    "    else:\n",
    "        conn_node = syntax_tree.get_common_ancestor_by_token_indices(conn_indices)\n",
    "        conn_leaves = set([syntax_tree.get_leaf_node_by_token_index(conn_index) for conn_index in conn_indices])\n",
    "        children = conn_node.get_children()\n",
    "        for child in children:\n",
    "            leaves = set(child.get_leaves())\n",
    "            if conn_leaves & leaves == set([]):\n",
    "                constituent_nodes.append(child)\n",
    "\n",
    "    curr = conn_node\n",
    "    while not curr.is_root():\n",
    "        constituent_nodes.extend(syntax_tree.get_siblings(curr))\n",
    "        curr = curr.up\n",
    "\n",
    "    # obtain the Constituent object according to the node.\n",
    "    constituents = []\n",
    "    for node in constituent_nodes:\n",
    "        cons = Constituent(syntax_tree, node)\n",
    "#         cons.connective = connective\n",
    "        constituents.append(cons)\n",
    "    return constituents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_tree = \"( (S (S (NP (DT Some)) (VP (MD may) (VP (VB have) (VP (VBN forgotten))))) (: --) (CC and) (S (NP (DT some) (JJR younger) (NNS ones)) (VP (MD may) (ADVP (RB never)) (VP (VB have) (ADJP (JJ experienced)) (: --) (SBAR (WHNP (WP what)) (S (NP (PRP it)) (VP (VBZ 's) (VP (VB like) (S (VP (TO to) (VP (VB invest) (PP (IN during) (NP (DT a) (NN recession))))))))))))) (. .)) )\"\n",
    "# all_tree = get_all_tree(parse_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_train = '/home/pengfei/data/2015-2016_conll_shared_task/data/conll16st-en-03-29-16-train/pdtb-parses.json'\n",
    "parse_dict_train = json.loads(codecs.open(conll_train, encoding='utf-8', errors='ignore').read())\n",
    "conll_dev = '/home/pengfei/data/2015-2016_conll_shared_task/data/conll16st-en-03-29-16-dev/pdtb-parses.json'\n",
    "parse_dict_dev = json.loads(codecs.open(conll_dev, encoding='utf-8', errors='ignore').read())\n",
    "conll_test = '/home/pengfei/data/2015-2016_conll_shared_task/data/conll16st-en-03-29-16-test/pdtb-parses.json'\n",
    "parse_dict_test = json.loads(codecs.open(conll_test, encoding='utf-8', errors='ignore').read())\n",
    "parse_dict = merge3dicts(parse_dict_train, parse_dict_dev, parse_dict_test)\n",
    "\n",
    "parse_data_path = \"/home/pengfei/data/2015-2016_conll_shared_task/data/conll16st-en-03-29-16-train/relations.json\"\n",
    "parse_data_dev_path = '/home/pengfei/data/2015-2016_conll_shared_task/data/conll16st-en-03-29-16-dev/relations.json'\n",
    "parse_data_test_path = '/home/pengfei/data/2015-2016_conll_shared_task/data/conll16st-en-03-29-16-test/relations.json'\n",
    "parse_data = [json.loads(line) for line in codecs.open(parse_data_path).readlines()]\n",
    "parse_data_dev = [json.loads(line) for line in codecs.open(parse_data_dev_path).readlines()]\n",
    "parse_data_test = [json.loads(line) for line in codecs.open(parse_data_test_path).readlines()]\n",
    "parse_data.extend(parse_data_dev)\n",
    "parse_data.extend(parse_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cons = _get_constituents(parse_dict, 'wsj_0279', 11)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3874, 623, 0.8614631976873471, 111.72203691349789)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true = 0\n",
    "false = 0\n",
    "length = 0\n",
    "count = 0\n",
    "for r in parse_data[:10000]:\n",
    "    if r['Type'] == 'Explicit':\n",
    "        sent_index = list(set([o[3] for o in r['Arg2']['TokenList']]))\n",
    "        if len(sent_index) == 1:\n",
    "            sent_index = sent_index[0]\n",
    "            conn_indices = [o[4] for o in r['Connective']['TokenList']]\n",
    "            constituents = _get_constituents(parse_dict, r['DocID'], sent_index, conn_indices)\n",
    "            constituents = sorted(constituents, key=lambda constituent: constituent.indices[0])   # sort by age\n",
    "            first_level = [constituents[i].indices for i in range(len(constituents))]\n",
    "#             second_level = [constituents[i].indices + constituents[i+1].indices for i in range(len(constituents)-1)]\n",
    "            second_level = [constituents[i].indices + constituents[j].indices for i in range(len(constituents)) for j in range(len(constituents)) if i<j]\n",
    "            third_level = []\n",
    "            for i in range(2, len(constituents)):\n",
    "                for j in range(1, i):\n",
    "                    for k in range(j):\n",
    "                        third_level.append(constituents[k].indices + constituents[j].indices + constituents[i].indices)\n",
    "#             third_level = [constituents[i].indices + constituents[i+1].indices + constituents[i+2].indices for i in range(len(constituents)-2)]\n",
    "            fourth_level = [constituents[i].indices + constituents[i+1].indices + constituents[i+2].indices + constituents[i+3].indices for i in range(len(constituents)-3)]\n",
    "            fifth_level = [constituents[i].indices + constituents[i+1].indices + constituents[i+2].indices + constituents[i+3].indices + constituents[i+4].indices for i in range(len(constituents)-4)]\n",
    "            sixth_level = [constituents[i].indices + constituents[i+1].indices + constituents[i+2].indices + constituents[i+3].indices + constituents[i+4].indices + constituents[i+5].indices for i in range(len(constituents)-5)]\n",
    "            constituents = first_level + second_level + third_level + fourth_level + fifth_level + sixth_level\n",
    "            length += len(constituents)\n",
    "            count += 1\n",
    "            token_index = [o[4] for o in r['Arg2']['TokenList']]\n",
    "            if token_index in constituents:\n",
    "                true += 1\n",
    "            else:\n",
    "                false += 1\n",
    "\n",
    "true, false, true / (true + false), length / count\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6274509803921569"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true / (true + false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets loaded\n"
     ]
    }
   ],
   "source": [
    "conll_train = '/home/pengfei/data/PDTB-3.0/all/conll/train/pdtb-parses.json'\n",
    "parse_dict_train = json.loads(codecs.open(conll_train, encoding='utf-8', errors='ignore').read())\n",
    "conll_dev = '/home/pengfei/data/PDTB-3.0/all/conll/dev/pdtb-parses.json'\n",
    "parse_dict_dev = json.loads(codecs.open(conll_dev, encoding='utf-8', errors='ignore').read())\n",
    "conll_test = '/home/pengfei/data/PDTB-3.0/all/conll/test/pdtb-parses.json'\n",
    "parse_dict_test = json.loads(codecs.open(conll_test, encoding='utf-8', errors='ignore').read())\n",
    "print(\"datasets loaded\")\n",
    "parse_dict = merge3dicts(parse_dict_train, parse_dict_dev, parse_dict_test)\n",
    "\n",
    "parse_data_path = \"/home/pengfei/data/PDTB-3.0/all/conll/train/relations.json\"\n",
    "parse_data_dev_path = '/home/pengfei/data/PDTB-3.0/all/conll/dev/relations.json'\n",
    "parse_data_test_path = '/home/pengfei/data/PDTB-3.0/all/conll/test/relations.json'\n",
    "parse_data = [json.loads(line) for line in codecs.open(parse_data_path).readlines()]\n",
    "parse_data_dev = [json.loads(line) for line in codecs.open(parse_data_dev_path).readlines()]\n",
    "parse_data_test = [json.loads(line) for line in codecs.open(parse_data_test_path).readlines()]\n",
    "parse_data.extend(parse_data_dev)\n",
    "parse_data.extend(parse_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(357, 98, 0.7846153846153846, 36.417582417582416, 455)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true = 0\n",
    "false = 0\n",
    "length = 0\n",
    "count = 0\n",
    "for r in parse_data[:1000]:\n",
    "    if r['Type'] == 'Explicit':\n",
    "        sent_index = list(set([o[3] for o in r['Arg2']['TokenList']]))\n",
    "        if len(sent_index) == 1 and r['Connective']['TokenList'][0][3] == sent_index[0]:\n",
    "            sent_index = sent_index[0]\n",
    "            conn_indices = [o[4] for o in r['Connective']['TokenList']]\n",
    "            constituents = _get_constituents(parse_dict, r['DocID'], sent_index, conn_indices)\n",
    "            constituents = sorted(constituents, key=lambda constituent: constituent.indices[0])   # sort by age\n",
    "            first_level = [constituents[i].indices for i in range(len(constituents))]\n",
    "    #             second_level = [constituents[i].indices + constituents[i+1].indices for i in range(len(constituents)-1)]\n",
    "    #             third_level = [constituents[i].indices + constituents[i+1].indices + constituents[i+2].indices for i in range(len(constituents)-2)]\n",
    "    #             fourth_level = [constituents[i].indices + constituents[i+1].indices + constituents[i+2].indices + constituents[i+3].indices for i in range(len(constituents)-3)]\n",
    "    #             fifth_level = [constituents[i].indices + constituents[i+1].indices + constituents[i+2].indices + constituents[i+3].indices + constituents[i+4].indices for i in range(len(constituents)-4)]\n",
    "    #             sixth_level = [constituents[i].indices + constituents[i+1].indices + constituents[i+2].indices + constituents[i+3].indices + constituents[i+4].indices + constituents[i+5].indices for i in range(len(constituents)-5)]\n",
    "            second_level = [constituents[i].indices + constituents[j].indices for i in range(len(constituents)) for j in range(len(constituents)) if i<j]\n",
    "    #             third_level = []\n",
    "    #             for i in range(2, len(constituents)):\n",
    "    #                 for j in range(1, i):\n",
    "    #                     for k in range(j):\n",
    "    #                         third_level.append(constituents[k].indices + constituents[j].indices + constituents[i].indices)\n",
    "            constituents = first_level + second_level #+ third_level + fourth_level + fifth_level + sixth_level\n",
    "            length += len(constituents)\n",
    "            count += 1\n",
    "            token_index = [o[4] for o in r['Arg2']['TokenList']]\n",
    "            if token_index in constituents:\n",
    "                true += 1\n",
    "            else:\n",
    "                false += 1\n",
    "    #                 print(token_index)\n",
    "    #                 print()\n",
    "    #                 dis = [len(set(token_index).symmetric_difference(set(o))) for o in constituents]\n",
    "    #                 print(constituents[dis.index(min(dis))])\n",
    "    #                 print(\"==========\")\n",
    "\n",
    "true, false, true / (true + false), length / count, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = 0\n",
    "false = 0\n",
    "length = 0\n",
    "count = 0\n",
    "for r in parse_data[:1000]:\n",
    "    if r['Type'] == 'Explicit':\n",
    "        sent_index = list(set([o[3] for o in r['Arg1']['TokenList']]))\n",
    "        if len(sent_index) == 1 and r['Connective']['TokenList'][0][3] == sent_index[0]:\n",
    "            sent_index = sent_index[0]\n",
    "            conn_indices = [o[4] for o in r['Connective']['TokenList']]\n",
    "            constituents = _get_constituents(parse_dict, r['DocID'], sent_index, conn_indices)\n",
    "            constituents = sorted(constituents, key=lambda constituent: constituent.indices[0])   # sort by age\n",
    "            first_level = [constituents[i].indices for i in range(len(constituents))]\n",
    "    #             second_level = [constituents[i].indices + constituents[i+1].indices for i in range(len(constituents)-1)]\n",
    "    #             third_level = [constituents[i].indices + constituents[i+1].indices + constituents[i+2].indices for i in range(len(constituents)-2)]\n",
    "    #             fourth_level = [constituents[i].indices + constituents[i+1].indices + constituents[i+2].indices + constituents[i+3].indices for i in range(len(constituents)-3)]\n",
    "    #             fifth_level = [constituents[i].indices + constituents[i+1].indices + constituents[i+2].indices + constituents[i+3].indices + constituents[i+4].indices for i in range(len(constituents)-4)]\n",
    "    #             sixth_level = [constituents[i].indices + constituents[i+1].indices + constituents[i+2].indices + constituents[i+3].indices + constituents[i+4].indices + constituents[i+5].indices for i in range(len(constituents)-5)]\n",
    "            second_level = [constituents[i].indices + constituents[j].indices for i in range(len(constituents)) for j in range(len(constituents)) if i<j]\n",
    "    #             third_level = []\n",
    "    #             for i in range(2, len(constituents)):\n",
    "    #                 for j in range(1, i):\n",
    "    #                     for k in range(j):\n",
    "    #                         third_level.append(constituents[k].indices + constituents[j].indices + constituents[i].indices)\n",
    "            constituents = first_level + second_level #+ third_level + fourth_level + fifth_level + sixth_level\n",
    "            length += len(constituents)\n",
    "            count += 1\n",
    "            token_index = [o[4] for o in r['Arg2']['TokenList']]\n",
    "            if token_index in constituents:\n",
    "                true += 1\n",
    "            else:\n",
    "                false += 1\n",
    "    #                 print(token_index)\n",
    "    #                 print()\n",
    "    #                 dis = [len(set(token_index).symmetric_difference(set(o))) for o in constituents]\n",
    "    #                 print(constituents[dis.index(min(dis))])\n",
    "    #                 print(\"==========\")\n",
    "\n",
    "true, false, true / (true + false), length / count, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from pdtb_api.api import PDTB3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/pengfei/PDTB-3.0/all/conll/train/relations.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fc2a0f083dc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpdtb3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPDTB3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Github/pdtb3-mapper/pdtb_api/api.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mdata_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mto\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mconll\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \"\"\"\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_data_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'train/relations.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_dict_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'train/pdtb-parses.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py37/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, encoding, errors, buffering)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;31m# Force opening of the file in binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/pengfei/PDTB-3.0/all/conll/train/relations.json'"
     ]
    }
   ],
   "source": [
    "pdtb3 = PDTB3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'/home/pengfei/'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
