{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from api.api import PDTB\n",
    "from constituent import Constituent\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdtb2 = '/home/pengfei/data/pdtb_v2/all/conll/'\n",
    "pdtb3 = PDTB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## only for one individual sentence for explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_is_contained(34, 'Arg1')\n",
    "# pdtb3.get_arg_token_list(6, 'Arg1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11437908496732026"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad = 0\n",
    "total =0\n",
    "for i in range(1000):\n",
    "    if pdtb3.parse_data[i]['Type'] == 'Explicit' and pdtb3.get_arg_sent_id(i,'Arg1')[0] == pdtb3.get_arg_sent_id(i,'Arg2')[0]:\n",
    "        total+=1\n",
    "        if not pdtb3.arg_is_contained(i, 'Arg1'):\n",
    "            bad += 1\n",
    "#             print(pdtb3.get_highlighted_parsetree(i, v=True))\n",
    "1- bad/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9313725490196079"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad = 0\n",
    "total =0\n",
    "for i in range(1000):\n",
    "    if pdtb3.parse_data[i]['Type'] == 'Explicit' and pdtb3.get_arg_sent_id(i,'Arg1')[0] == pdtb3.get_arg_sent_id(i,'Arg2')[0]:\n",
    "        total+=1\n",
    "        if not pdtb3.arg_is_contained(i, 'Arg2'):\n",
    "            bad += 1\n",
    "#             print(pdtb3.get_highlighted_parsetree(i, v=True))\n",
    "1- bad/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for one individual sentence for implicit\n",
    "# for PS case for explicit\n",
    "# for PS case for implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DocID': 'wsj_0799', 'ID': 106, 'Type': 'Explicit', 'Sense': ['Expansion.Conjunction'], 'Connective': {'CharacterSpanList': [[5809, 5817], [5878, 5886]], 'RawText': 'not only but also', 'TokenList': [[5809, 5812, 1089, 46, 2], [5813, 5817, 1090, 46, 3], [5878, 5881, 1097, 46, 10], [5882, 5886, 1098, 46, 11]]}, 'Arg1': {'CharacterSpanList': [[5818, 5876]], 'RawText': 'merged three military-electronics manufacturing operations', 'TokenList': [[5818, 5824, 1091, 46, 4], [5825, 5830, 1092, 46, 5], [5831, 5851, 1093, 46, 6], [5852, 5865, 1094, 46, 7], [5866, 5876, 1095, 46, 8]]}, 'Arg2': {'CharacterSpanList': [[5887, 5976]], 'RawText': 'closed an unrelated plant that makes ordnance devices used in fighter planes and missiles', 'TokenList': [[5887, 5893, 1099, 46, 12], [5894, 5896, 1100, 46, 13], [5897, 5906, 1101, 46, 14], [5907, 5912, 1102, 46, 15], [5913, 5917, 1103, 46, 16], [5918, 5923, 1104, 46, 17], [5924, 5932, 1105, 46, 18], [5933, 5940, 1106, 46, 19], [5941, 5945, 1107, 46, 20], [5946, 5948, 1108, 46, 21], [5949, 5956, 1109, 46, 22], [5957, 5963, 1110, 46, 23], [5964, 5967, 1111, 46, 24], [5968, 5976, 1112, 46, 25]]}}\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "for i, r in enumerate(pdtb3.parse_data):\n",
    "    if r['Type'] != 'Explicit':\n",
    "        continue\n",
    "    if len(r['Connective']['TokenList']) > 2:\n",
    "        print(r)\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _get_constituents(docid, sentid, conn_index):\n",
    "#     \"\"\"\n",
    "#     Args:\n",
    "#             docid(str)\n",
    "#             sentid(int)\n",
    "#             conn_index(list[int])\n",
    "#     Returns:\n",
    "#             [ [(token_index, text),...], ...]\"\"\"\n",
    "#     syntax_tree = pdtb3.get_syntax_tree(docid, sentid)\n",
    "#     if syntax_tree.tree == None:\n",
    "#         return []\n",
    "#     all_leaves = syntax_tree.tree.get_leaves()\n",
    "#     conn_indices = conn_index\n",
    "#     constituent_nodes = []\n",
    "#     if len(conn_indices) == 1:# like and or so...\n",
    "#         conn_node = syntax_tree.get_leaf_node_by_token_index(conn_indices[0]).up\n",
    "#     else:\n",
    "#         conn_node = syntax_tree.get_common_ancestor_by_token_indices(conn_indices)\n",
    "#         conn_leaves = set([all_leaves.index(syntax_tree.get_leaf_node_by_token_index(conn_index)) for conn_index in conn_indices])\n",
    "#         children = conn_node.get_children()\n",
    "#         for child in children:\n",
    "#             leaves = set([all_leaves.index(n) for n in child.get_leaves()])\n",
    "#             if list(leaves-conn_leaves)!=[]: constituent_nodes.append(list(leaves-conn_leaves))\n",
    "#     curr = conn_node\n",
    "#     while not curr.is_root():\n",
    "#         sibs = [ [all_leaves.index(n) for n in sib] for sib in syntax_tree.get_siblings(curr)]\n",
    "#         constituent_nodes.extend(sibs)\n",
    "#         curr = curr.up\n",
    "\n",
    "#     tokens_indices_with_text = pdtb3.get_tokens_indices_with_text(docid, sentid)\n",
    "#     subtree_list = [ [(o,tokens_indices_with_text[o][2]) for o in node] for node in constituent_nodes]\n",
    "#     return subtree_list\n",
    "\n",
    "# def merge_consti(subtree_list):\n",
    "#     return {(o[0][0],o[-1][0]):i for i,o in enumerate(subtree_list)}\n",
    "\n",
    "# def arg_is_contained(i, arg):\n",
    "#     docid = pdtb3.parse_data[i]['DocID']\n",
    "#     sentid = pdtb3.get_arg_sent_id(i, arg)\n",
    "#     conn_indices = [o[1] for o in pdtb3.get_arg_token_list(i, 'Connective')]\n",
    "#     subtree_list = _get_constituents(docid, sentid[0], conn_indices)\n",
    "#     merged_consti_list = merge_consti(subtree_list)\n",
    "#     token_list = pdtb3.get_arg_token_list(i, arg)\n",
    "#     start,end = token_list[0][1], token_list[-1][1]\n",
    "#     # filter results\n",
    "#     consti_list = [k for k in merged_consti_list.keys() if k[0]>=start and k[1]<=end]\n",
    "#     # generate results\n",
    "#     results = []\n",
    "#     for i in range(1, len(consti_list)+1):\n",
    "#         for subset in itertools.combinations(consti_list, i):\n",
    "#             results.append(expand(subset))\n",
    "#     # check\n",
    "#     return [o[1] for o in token_list] in results\n",
    "\n",
    "# def expand(subset):\n",
    "#     ret = []\n",
    "#     for subsubset in subset:\n",
    "#         for i in range(subsubset[0], subsubset[1]+1):\n",
    "#             ret.append(i)\n",
    "#     return sorted(ret)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
