{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "import codecs\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path_3 = '/home/pengfei/data/PDTB-3.0/data/raw/'\n",
    "raw_data_path_2 = '/home/pengfei/data/2015-2016_conll_shared_task/raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_3 = os.listdir(raw_data_path_3)\n",
    "folder_2 = os.listdir(raw_data_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_3 = set([filename for folder in folder_3 for filename in os.listdir(raw_data_path_3 + folder)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_2 = set(folder_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_2 - file_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_3 - file_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_2 - file_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conclusion:\n",
    "conll version does not contain 00, 01, and 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_data_path_3 = '/home/pengfei/data/PDTB-3.0/data/gold/'\n",
    "gold_data_path_2 = '/home/pengfei/data/2015-2016_conll_shared_task/data/conll16st-en-03-29-16-train/pdtb-parses.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_3 = os.listdir(gold_data_path_3)\n",
    "file_3 = set([filename for folder in folder_3 for filename in os.listdir(gold_data_path_3 + folder)])\n",
    "parse_dict = json.loads(codecs.open(gold_data_path_2, encoding='utf-8', errors='ignore').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 = set(parse_dict.keys())\n",
    "file3 = file_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conclusion:\n",
    "file3 contain files that is not in file2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(o[4:6] for o in (file2 - file3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'00', '01', '09', '12', '13', '22', '23', '24'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(o[4:6] for o in (file3 - file2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsj_1344\n",
      "wsj_0998\n",
      "wsj_1202\n"
     ]
    }
   ],
   "source": [
    "for o in file3-file2:\n",
    "    if o[4:6] in ['09', '12', '13']:\n",
    "        print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pdtb study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocID</th>\n",
       "      <th>Relation_Type</th>\n",
       "      <th>Conn_SpanList</th>\n",
       "      <th>Conn_Src</th>\n",
       "      <th>Conn_Type</th>\n",
       "      <th>Conn_Pol</th>\n",
       "      <th>Conn_Det</th>\n",
       "      <th>Conn_Feat_SpanList</th>\n",
       "      <th>Conn1</th>\n",
       "      <th>SClass1A</th>\n",
       "      <th>...</th>\n",
       "      <th>Arg2_Det</th>\n",
       "      <th>Arg2_Feat_SpanList</th>\n",
       "      <th>Sup2_SpanList</th>\n",
       "      <th>Adju_Reason</th>\n",
       "      <th>Adju_Disagr</th>\n",
       "      <th>PB_Role</th>\n",
       "      <th>PB_Verb</th>\n",
       "      <th>Offset</th>\n",
       "      <th>Provenance</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wsj_0793</td>\n",
       "      <td>EntRel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>166</td>\n",
       "      <td>PDTB2::wsj_0793::166::SAME</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wsj_0793</td>\n",
       "      <td>Explicit</td>\n",
       "      <td>197..204</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>instead</td>\n",
       "      <td>Expansion.Substitution.Arg2-as-subst</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>197..204</td>\n",
       "      <td>PDTB2::wsj_0793::197..204::CHANGED</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wsj_0793</td>\n",
       "      <td>Implicit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>then</td>\n",
       "      <td>Temporal.Asynchronous.Precedence</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>281</td>\n",
       "      <td>PDTB3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wsj_0793</td>\n",
       "      <td>Implicit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>as a result</td>\n",
       "      <td>Contingency.Cause.Result</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>333</td>\n",
       "      <td>PDTB2::wsj_0793::333::SAME</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wsj_0793</td>\n",
       "      <td>Implicit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>but</td>\n",
       "      <td>Comparison.Concession.Arg2-as-denier</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>740</td>\n",
       "      <td>PDTB2::wsj_0793::740::CHANGED</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      DocID Relation_Type Conn_SpanList  Conn_Src  Conn_Type  Conn_Pol  \\\n",
       "0  wsj_0793        EntRel           NaN       NaN        NaN       NaN   \n",
       "1  wsj_0793      Explicit      197..204       NaN        NaN       NaN   \n",
       "2  wsj_0793      Implicit           NaN       NaN        NaN       NaN   \n",
       "3  wsj_0793      Implicit           NaN       NaN        NaN       NaN   \n",
       "4  wsj_0793      Implicit           NaN       NaN        NaN       NaN   \n",
       "\n",
       "   Conn_Det Conn_Feat_SpanList        Conn1  \\\n",
       "0       NaN                NaN          NaN   \n",
       "1       NaN                NaN      instead   \n",
       "2       NaN                NaN         then   \n",
       "3       NaN                NaN  as a result   \n",
       "4       NaN                NaN          but   \n",
       "\n",
       "                               SClass1A  ... Arg2_Det Arg2_Feat_SpanList  \\\n",
       "0                                   NaN  ...      NaN                NaN   \n",
       "1  Expansion.Substitution.Arg2-as-subst  ...      NaN                NaN   \n",
       "2      Temporal.Asynchronous.Precedence  ...      NaN                NaN   \n",
       "3              Contingency.Cause.Result  ...      NaN                NaN   \n",
       "4  Comparison.Concession.Arg2-as-denier  ...      NaN                NaN   \n",
       "\n",
       "  Sup2_SpanList  Adju_Reason Adju_Disagr PB_Role  PB_Verb    Offset  \\\n",
       "0           NaN          NaN         NaN     NaN      NaN       166   \n",
       "1           NaN          NaN         NaN     NaN      NaN  197..204   \n",
       "2           NaN          NaN         NaN     NaN      NaN       281   \n",
       "3           NaN          NaN         NaN     NaN      NaN       333   \n",
       "4           NaN          NaN         NaN     NaN      NaN       740   \n",
       "\n",
       "                           Provenance  Link  \n",
       "0          PDTB2::wsj_0793::166::SAME   NaN  \n",
       "1  PDTB2::wsj_0793::197..204::CHANGED   NaN  \n",
       "2                               PDTB3   NaN  \n",
       "3          PDTB2::wsj_0793::333::SAME   NaN  \n",
       "4       PDTB2::wsj_0793::740::CHANGED   NaN  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdtb3 = pd.read_csv('pdtb3.csv')\n",
    "pdtb3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Implicit', 'EntRel', 'NoRel', 'Hypophora', 'AltLexC', 'AltLex', 'Explicit'}\n",
      "Index(['Relation_Type', 'Conn_SpanList', 'Conn_Src', 'Conn_Type', 'Conn_Pol',\n",
      "       'Conn_Det', 'Conn_Feat_SpanList', 'Conn1', 'SClass1A', 'SClass1B',\n",
      "       'Conn2', 'SClass2A', 'SClass2B', 'Sup1_SpanList', 'Arg1_SpanList',\n",
      "       'Arg1_Src', 'Arg1_Type', 'Arg1_Pol', 'Arg1_Det', 'Arg2_Feat_SpanList',\n",
      "       'Arg2_SpanList', 'Arg2_Src', 'Arg2_Type', 'Arg2_Pol', 'Arg2_Det',\n",
      "       'Arg2_Feat_SpanList.1', 'Sup2_SpanList', 'Adju_Reason', 'Adju_Disagr',\n",
      "       'PB_Role', 'PB_Verb', 'Offset', 'Provenance', 'Link'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "Type = []\n",
    "for i in range(len(pdtb3)):\n",
    "    Type.append(pdtb3.loc[i, 'Relation_Type'])\n",
    "print(set(Type)), \n",
    "print(pdtb3.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pdtb3)):\n",
    "    if pdtb3.loc[i, 'Relation_Type'] == 'Explicit':\n",
    "        assert(type(pdtb3.loc[i, 'Conn_SpanList']) != float)\n",
    "        assert(type(pdtb3.loc[i, 'Arg1_SpanList']) != float)\n",
    "        assert(type(pdtb3.loc[i, 'Arg2_SpanList']) != float)\n",
    "        assert(type(pdtb3.loc[i, 'Conn1']) != float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pdtb3)):\n",
    "    if pdtb3.loc[i, 'Relation_Type'] == 'AltLex':\n",
    "        assert(type(pdtb3.loc[i, 'Conn_SpanList']) != float)\n",
    "        assert(type(pdtb3.loc[i, 'Conn1']) == float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pdtb3)):\n",
    "    if pdtb3.loc[i, 'Relation_Type'] == 'AltLexC':\n",
    "        assert(type(pdtb3.loc[i, 'Conn_SpanList']) != float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pdtb3)):\n",
    "    if pdtb3.loc[i, 'Relation_Type'] == 'Implicit':\n",
    "        assert(type(pdtb3.loc[i, 'Conn_SpanList']) == float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pdtb3)):\n",
    "    if pdtb3.loc[i, 'Relation_Type'] == 'EntRel':\n",
    "        assert(type(pdtb3.loc[i, 'Conn_SpanList']) == float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pdtb3)):\n",
    "    if pdtb3.loc[i, 'Relation_Type'] == 'NoRel':\n",
    "        assert(type(pdtb3.loc[i, 'Conn_SpanList']) == float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pdtb3)):\n",
    "    if pdtb3.loc[i, 'Relation_Type'] == 'Hypophora':\n",
    "        assert(type(pdtb3.loc[i, 'Conn_SpanList']) == float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 LINK1\n"
     ]
    }
   ],
   "source": [
    "check = 'Link'\n",
    "for i in range(len(pdtb3)):\n",
    "    if not (type(pdtb3.loc[i, check]) == np.float64 or type(pdtb3.loc[i, check]) == float): \n",
    "        print(i, pdtb3.loc[i, check])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for conn span list: number of uncorrected 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "till 1612 wsj_0771 'til\n",
      "moreover 5315 wsj_0619 morever\n",
      "because 51998 wsj_2215 'cause\n",
      "moreover 52804 wsj_2202 morever\n",
      "Number of incorrect conn span list 4\n"
     ]
    }
   ],
   "source": [
    "main_folder = '/home/pengfei/data/PDTB-3.0/all/raw/'\n",
    "false_conn_list = []\n",
    "def get_span_list(span):\n",
    "    if span == '':\n",
    "        assert(False)\n",
    "    spans = span.split(';')\n",
    "    return [[int(k) for k in o.split('..')] for o in spans if o != '']\n",
    "\n",
    "\n",
    "for i in range(len(pdtb3)):\n",
    "    if pdtb3.loc[i, 'Relation_Type'] == 'Explicit':\n",
    "        spanlist = get_span_list(pdtb3.loc[i, 'Conn_SpanList'])\n",
    "        with open(main_folder + pdtb3.loc[i, 'DocID']) as f:\n",
    "            rawtext = f.read()\n",
    "            expected = ' '.join([rawtext[o[0]: o[1]] for o in spanlist]).lower()\n",
    "        if pdtb3.loc[i, 'Conn1'] not in expected:\n",
    "            print(pdtb3.loc[i, 'Conn1'], i, pdtb3.loc[i, 'DocID'], expected)\n",
    "            false_conn_list.append(i)\n",
    "            \n",
    "print(\"Number of incorrect conn span list\", len(false_conn_list))\n",
    "\n",
    "for i in false_conn_list:\n",
    "    with open(main_folder+pdtb3.loc[i,'DocID']) as f:\n",
    "        rawtext = f.read().lower()\n",
    "        start = int(pdtb3.loc[i, 'Conn_SpanList'].split('..')[0])\n",
    "        distance = [abs(m.start() - start) for m in re.finditer(pdtb3.loc[i,'Conn1'], rawtext, )]\n",
    "        start = [m.start() for m in re.finditer(pdtb3.loc[i, 'Conn1'], rawtext)]\n",
    "        if distance != []:\n",
    "            index = distance.index(min(distance))\n",
    "            start = start[index]\n",
    "            span = str(start) + '..' + str(start+len(pdtb3.loc[i, 'Conn1']))\n",
    "            pdtb3.loc[i, 'Conn_SpanList'] = span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check all the explicit connectives is in the connectives list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check all raw data doesn't change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = Path('/home/pengfei/data/pdtb_v2/all/raw')\n",
    "filenames = [o for o in os.listdir(main_folder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdtb2_raw_folder = main_folder\n",
    "pdtb3_raw_folder = Path('/home/pengfei/data/PDTB-3.0/all/raw')\n",
    "\n",
    "raw_false = []\n",
    "for filename in filenames:\n",
    "    with open(pdtb2_raw_folder/filename) as f:\n",
    "        raw2 = f.read()\n",
    "    with open(pdtb3_raw_folder/filename) as f:\n",
    "        raw3 = f.read()\n",
    "    if raw2 != raw3:\n",
    "        raw_false.append(filename)\n",
    "#         print(raw2.replace('\\n', ''))\n",
    "#         print()\n",
    "#         print(raw3.replace('\\n', ''))\n",
    "#         print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## raw data does change adding space dot and \\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for Arg1 spanlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_data_path_2 = '/home/pengfei/data/2015-2016_conll_shared_task/data/conll16st-en-03-29-16-train/pdtb-parses.json'\n",
    "parse_dict = json.loads(codecs.open(gold_data_path_2, encoding='utf-8', errors='ignore').read())\n",
    "main_folder = '/home/pengfei/data/PDTB-3.0/all/raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of Arg1 error 90\n",
      "{'wsj_2112', 'wsj_1248', 'wsj_1554', 'wsj_1830', 'wsj_1234', 'wsj_2086', 'wsj_1903', 'wsj_1201', 'wsj_1274', 'wsj_0597', 'wsj_1373', 'wsj_0448', 'wsj_1310', 'wsj_1044', 'wsj_1057', 'wsj_1630', 'wsj_0557', 'wsj_1303', 'wsj_1735', 'wsj_1986', 'wsj_1255', 'wsj_1515', 'wsj_1618', 'wsj_1924', 'wsj_1641', 'wsj_1855', 'wsj_0338', 'wsj_1915', 'wsj_1037', 'wsj_1744', 'wsj_1318', 'wsj_0322', 'wsj_0400', 'wsj_1125', 'wsj_0445', 'wsj_1492', 'wsj_1794', 'wsj_2172', 'wsj_1787', 'wsj_1473', 'wsj_0435', 'wsj_0567', 'wsj_1225', 'wsj_1916', 'wsj_1250', 'wsj_0328', 'wsj_0245', 'wsj_1048', 'wsj_1363', 'wsj_1372', 'wsj_1249', 'wsj_1289', 'wsj_0284', 'wsj_0910', 'wsj_0786', 'wsj_2070', 'wsj_0589', 'wsj_0776', 'wsj_0240', 'wsj_0367', 'wsj_1341', 'wsj_1754', 'wsj_2105', 'wsj_1377', 'wsj_2071', 'wsj_1247', 'wsj_1586'}\n"
     ]
    }
   ],
   "source": [
    "def get_list(parse_dict, doc):\n",
    "    doc = parse_dict[doc]['sentences']\n",
    "    char_start_list = []\n",
    "    char_end_list = []\n",
    "    for sentence in doc:\n",
    "        for word in sentence['words']:\n",
    "            char_start_list.append(word[1]['CharacterOffsetBegin'])\n",
    "            char_end_list.append(word[1]['CharacterOffsetEnd'])\n",
    "    return char_start_list, char_end_list\n",
    "\n",
    "count = 0\n",
    "doc_list = []\n",
    "for i in range(len(pdtb3)):\n",
    "    if pdtb3.loc[i, 'Relation_Type'] == 'Explicit' and pdtb3.loc[i, 'DocID'] in parse_dict.keys():\n",
    "        char_start_list, char_end_list = get_list(parse_dict, pdtb3.loc[i, 'DocID'])\n",
    "        spanlist = get_span_list(pdtb3.loc[i, 'Arg1_SpanList'])\n",
    "        for span in spanlist:\n",
    "            if not ((span[0] in char_start_list) and (span[1] in char_end_list)):\n",
    "                count += 1\n",
    "                doc_list.append(pdtb3.loc[i, 'DocID'])\n",
    "#                 with open(main_folder  + pdtb3.loc[i, 'DocID']) as f:\n",
    "#                     rawtext = f.read()\n",
    "#                     expected = ' '.join([rawtext[o[0]: o[1]] for o in spanlist])\n",
    "print(\"the number of Arg1 error\", count)\n",
    "print(set(doc_list) - set(raw_false))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 90)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(doc_list)), len(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(doc_list) - set(raw_false))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for arg2 spanlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of Arg1 error 77\n"
     ]
    }
   ],
   "source": [
    "def get_list(parse_dict, doc):\n",
    "    doc = parse_dict[doc]['sentences']\n",
    "    char_start_list = []\n",
    "    char_end_list = []\n",
    "    for sentence in doc:\n",
    "        for word in sentence['words']:\n",
    "            char_start_list.append(word[1]['CharacterOffsetBegin'])\n",
    "            char_end_list.append(word[1]['CharacterOffsetEnd'])\n",
    "    return char_start_list, char_end_list\n",
    "\n",
    "count = 0\n",
    "for i in range(len(pdtb3)):\n",
    "    if pdtb3.loc[i, 'Relation_Type'] == 'Explicit' and pdtb3.loc[i, 'DocID'] in parse_dict.keys():\n",
    "        char_start_list, char_end_list = get_list(parse_dict, pdtb3.loc[i, 'DocID'])\n",
    "        spanlist = get_span_list(pdtb3.loc[i, 'Arg2_SpanList'])\n",
    "        for span in spanlist:\n",
    "            if not ((span[0] in char_start_list) and (span[1] in char_end_list)):\n",
    "                count += 1\n",
    "#                 print(i, pdtb3.loc[i, 'DocID'], spanlist)\n",
    "\n",
    "print(\"the number of Arg1 error\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for pdtb2 arg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of Arg1 error 14940\n"
     ]
    }
   ],
   "source": [
    "pdtb2 = pd.read_csv('/home/pengfei/Github/pdtb2/pdtb2.csv', low_memory=False)\n",
    "gold_data_path_2 = '/home/pengfei/data/2015-2016_conll_shared_task/data/conll16st-en-03-29-16-train/pdtb-parses.json'\n",
    "parse_dict = json.loads(codecs.open(gold_data_path_2, encoding='utf-8', errors='ignore').read())\n",
    "def get_list(parse_dict, doc):\n",
    "    doc = parse_dict[doc]['sentences']\n",
    "    char_start_list = []\n",
    "    char_end_list = []\n",
    "    for sentence in doc:\n",
    "        for word in sentence['words']:\n",
    "            char_start_list.append(word[1]['CharacterOffsetBegin'])\n",
    "            char_end_list.append(word[1]['CharacterOffsetEnd'])\n",
    "    return char_start_list, char_end_list\n",
    "\n",
    "def get_docid(pdtb2, i):\n",
    "    return 'wsj_' + '{:02}'.format(pdtb2.loc[i, 'Section']) + '{:02}'.format(pdtb2.loc[i, 'FileNumber'])\n",
    "\n",
    "count = 0\n",
    "for i in range(len(pdtb2)):\n",
    "    if pdtb2.loc[i, 'Relation'] == 'Explicit' and get_docid(pdtb2, i) in parse_dict.keys():\n",
    "        char_start_list, char_end_list = get_list(parse_dict, get_docid(pdtb2, i))\n",
    "        spanlist = get_span_list(pdtb3.loc[i, 'Arg1_SpanList'])\n",
    "        for span in spanlist:\n",
    "            if not ((span[0] in char_start_list) and (span[1] in char_end_list)):\n",
    "                count += 1\n",
    "print(\"the number of Arg1 error\", count)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the use of 'Offset' is not clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the use of 'Link' is not clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = '/home/pengfei/data/PDTB-3.0/all/raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(pdtb3)):\n",
    "    pass\n",
    "#     if type(pdtb3.loc[i, 'Offset']) != float and pdtb3.loc[i, 'Relation_Type'] == 'Explicit':\n",
    "#         if pdtb3.loc[i, 'Offset'] not in pdtb3.loc[i, 'Conn_SpanList']:\n",
    "#             print(pdtb3.loc[i, 'Offset'], pdtb3.loc[i, 'Conn_SpanList'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
