{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how to solve inter hyphen issue https://stackoverflow.com/questions/52293874/why-does-spacy-not-preserve-intra-word-hyphens-during-tokenization-like-stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengfei/miniconda3/envs/benepar/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/pengfei/miniconda3/envs/benepar/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/pengfei/miniconda3/envs/benepar/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/pengfei/miniconda3/envs/benepar/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/pengfei/miniconda3/envs/benepar/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/pengfei/miniconda3/envs/benepar/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/pengfei/miniconda3/envs/benepar/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/pengfei/miniconda3/envs/benepar/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/pengfei/miniconda3/envs/benepar/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/pengfei/miniconda3/envs/benepar/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/pengfei/miniconda3/envs/benepar/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/pengfei/miniconda3/envs/benepar/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pengfei/miniconda3/envs/benepar/lib/python3.7/site-packages/benepar/base_parser.py:197: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/pengfei/miniconda3/envs/benepar/lib/python3.7/site-packages/benepar/base_parser.py:202: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
    "from benepar.spacy_plugin import BeneparComponent\n",
    "from spacy.tokens import Doc\n",
    "nlp = spacy.load('en')\n",
    "nlp.add_pipe(BeneparComponent('benepar_en2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_tokenizer(nlp):\n",
    "# #     infix_re = re.compile(r'''[~-]''')\n",
    "#     infix_re = compile_infix_regex(nlp.Defaults.infixes)\n",
    "#     prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "#     suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)\n",
    "\n",
    "#     return Tokenizer(nlp.vocab, \n",
    "#                      prefix_search=prefix_re.search,\n",
    "#                                 suffix_search=suffix_re.search,\n",
    "#                                 infix_finditer=infix_re.finditer,\n",
    "#                                 token_match=None)\n",
    "\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # with this:\n",
    "    return Doc(nlp.vocab, tokens)\n",
    "\n",
    "nlp.tokenizer = custom_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc(doc_path):\n",
    "    with open(doc_path) as f:\n",
    "        txt = f.read().replace('\\n', ' ')\n",
    "    doc = nlp(txt)\n",
    "    sentences = []\n",
    "    sent_token_offset = 0\n",
    "    for i, sent in enumerate(doc.sents):\n",
    "        if i == 0: continue\n",
    "        words = []\n",
    "        dependencies = []\n",
    "        count = 0\n",
    "        for word in sent:\n",
    "            charoffsetbegin = word.idx\n",
    "            charoffsetend = word.idx + len(word)\n",
    "            word_pos = word.pos_\n",
    "            word_string = str(word)\n",
    "            if str(word) == '\\\"':\n",
    "                if count % 2 == 0:\n",
    "                    word_string = '``'\n",
    "                else:\n",
    "                    word_string = '\\'\\''\n",
    "            word_combo = [word_string, \n",
    "                          {\"CharacterOffsetBegin\": charoffsetbegin, \n",
    "                                      \"CharacterOffsetEnd\": charoffsetend, \n",
    "                                      \"Linkers\": [], \n",
    "                                      \"PartOfSpeech\": word.pos_}]\n",
    "            words.append(word_combo)\n",
    "\n",
    "            dep_r = word.dep_\n",
    "            parent = str(word.head) + '-' + str(word.head.i - sent_token_offset)\n",
    "            child = str(word) + '-' + str(word.i - sent_token_offset)\n",
    "            dep = [dep_r, parent, child]\n",
    "            dependencies.append(dep)\n",
    "\n",
    "        parsetree = sent._.parse_string\n",
    "        parsetree = re.sub(r'\\(_SP ((.|\\n)*)\\)', '', parsetree)\n",
    "        sentences.append({\"parsetree\": parsetree, \"words\": words, \"dependencies\": dependencies})\n",
    "        sent_token_offset += len(sent)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/home/pengfei/data/pdtb3-dataset/all/raw/'\n",
    "files = os.listdir(folder_path)\n",
    "all_data = {}\n",
    "files = [f for f in files if f[4:6] in ['01', '00', '24']]\n",
    "files = ['wsj_0203',\n",
    " 'wsj_0205',\n",
    " 'wsj_0206',\n",
    " 'wsj_0212',\n",
    " 'wsj_0217',\n",
    " 'wsj_0226',\n",
    " 'wsj_0234',\n",
    " 'wsj_0249',\n",
    " 'wsj_0285',\n",
    " 'wsj_0320',\n",
    " 'wsj_0330',\n",
    " 'wsj_0366',\n",
    " 'wsj_0381',\n",
    " 'wsj_0382',\n",
    " 'wsj_0383',\n",
    " 'wsj_0384',\n",
    " 'wsj_0385',\n",
    " 'wsj_0386',\n",
    " 'wsj_0387',\n",
    " 'wsj_0388',\n",
    " 'wsj_0389',\n",
    " 'wsj_0390',\n",
    " 'wsj_0391',\n",
    " 'wsj_0392',\n",
    " 'wsj_0393',\n",
    " 'wsj_0394',\n",
    " 'wsj_0395',\n",
    " 'wsj_0396',\n",
    " 'wsj_0397',\n",
    " 'wsj_0398',\n",
    " 'wsj_0399',\n",
    " 'wsj_0425',\n",
    " 'wsj_0432',\n",
    " 'wsj_0438',\n",
    " 'wsj_0454',\n",
    " 'wsj_0455',\n",
    " 'wsj_0459',\n",
    " 'wsj_0463',\n",
    " 'wsj_0468',\n",
    " 'wsj_0506',\n",
    " 'wsj_0513',\n",
    " 'wsj_0517',\n",
    " 'wsj_0538',\n",
    " 'wsj_0543',\n",
    " 'wsj_0549',\n",
    " 'wsj_0555',\n",
    " 'wsj_0591',\n",
    " 'wsj_0620',\n",
    " 'wsj_0650',\n",
    " 'wsj_0678',\n",
    " 'wsj_0685',\n",
    " 'wsj_0691',\n",
    " 'wsj_0698',\n",
    " 'wsj_0699',\n",
    " 'wsj_0703',\n",
    " 'wsj_0707',\n",
    " 'wsj_0711',\n",
    " 'wsj_0715',\n",
    " 'wsj_0716',\n",
    " 'wsj_0720',\n",
    " 'wsj_0721',\n",
    " 'wsj_0724',\n",
    " 'wsj_0749',\n",
    " 'wsj_0821',\n",
    " 'wsj_0822',\n",
    " 'wsj_0823',\n",
    " 'wsj_0824',\n",
    " 'wsj_0825',\n",
    " 'wsj_0826',\n",
    " 'wsj_0827',\n",
    " 'wsj_0828',\n",
    " 'wsj_0829',\n",
    " 'wsj_0830',\n",
    " 'wsj_0831',\n",
    " 'wsj_0832',\n",
    " 'wsj_0833',\n",
    " 'wsj_0834',\n",
    " 'wsj_0835',\n",
    " 'wsj_0836',\n",
    " 'wsj_0837',\n",
    " 'wsj_0838',\n",
    " 'wsj_0839',\n",
    " 'wsj_0840',\n",
    " 'wsj_0841',\n",
    " 'wsj_0842',\n",
    " 'wsj_0843',\n",
    " 'wsj_0844',\n",
    " 'wsj_0845',\n",
    " 'wsj_0846',\n",
    " 'wsj_0847',\n",
    " 'wsj_0848',\n",
    " 'wsj_0849',\n",
    " 'wsj_0850',\n",
    " 'wsj_0851',\n",
    " 'wsj_0852',\n",
    " 'wsj_0853',\n",
    " 'wsj_0854',\n",
    " 'wsj_0855',\n",
    " 'wsj_0856',\n",
    " 'wsj_0857',\n",
    " 'wsj_0858',\n",
    " 'wsj_0859',\n",
    " 'wsj_0860',\n",
    " 'wsj_0861',\n",
    " 'wsj_0862',\n",
    " 'wsj_0863',\n",
    " 'wsj_0864',\n",
    " 'wsj_0865',\n",
    " 'wsj_0866',\n",
    " 'wsj_0867',\n",
    " 'wsj_0868',\n",
    " 'wsj_0869',\n",
    " 'wsj_0870',\n",
    " 'wsj_0871',\n",
    " 'wsj_0872',\n",
    " 'wsj_0873',\n",
    " 'wsj_0874',\n",
    " 'wsj_0875',\n",
    " 'wsj_0876',\n",
    " 'wsj_0877',\n",
    " 'wsj_0878',\n",
    " 'wsj_0879',\n",
    " 'wsj_0880',\n",
    " 'wsj_0881',\n",
    " 'wsj_0882',\n",
    " 'wsj_0883',\n",
    " 'wsj_0884',\n",
    " 'wsj_0885',\n",
    " 'wsj_0886',\n",
    " 'wsj_0887',\n",
    " 'wsj_0888',\n",
    " 'wsj_0889',\n",
    " 'wsj_0890',\n",
    " 'wsj_0891',\n",
    " 'wsj_0892',\n",
    " 'wsj_0893',\n",
    " 'wsj_0894',\n",
    " 'wsj_0895',\n",
    " 'wsj_0896',\n",
    " 'wsj_0897',\n",
    " 'wsj_0898',\n",
    " 'wsj_0899',\n",
    " 'wsj_0901',\n",
    " 'wsj_0920',\n",
    " 'wsj_0921',\n",
    " 'wsj_0947',\n",
    " 'wsj_0951',\n",
    " 'wsj_0998',\n",
    " 'wsj_1009',\n",
    " 'wsj_1063',\n",
    " 'wsj_1067',\n",
    " 'wsj_1077',\n",
    " 'wsj_1085',\n",
    " 'wsj_1087',\n",
    " 'wsj_1088',\n",
    " 'wsj_1090',\n",
    " 'wsj_1098',\n",
    " 'wsj_1104',\n",
    " 'wsj_1114',\n",
    " 'wsj_1115',\n",
    " 'wsj_1129',\n",
    " 'wsj_1197',\n",
    " 'wsj_1202',\n",
    " 'wsj_1240',\n",
    " 'wsj_1245',\n",
    " 'wsj_1277',\n",
    " 'wsj_1326',\n",
    " 'wsj_1342',\n",
    " 'wsj_1344',\n",
    " 'wsj_1378',\n",
    " 'wsj_1384',\n",
    " 'wsj_1385',\n",
    " 'wsj_1393',\n",
    " 'wsj_1400',\n",
    " 'wsj_1406',\n",
    " 'wsj_1420',\n",
    " 'wsj_1425',\n",
    " 'wsj_1427',\n",
    " 'wsj_1444',\n",
    " 'wsj_1482',\n",
    " 'wsj_1559',\n",
    " 'wsj_1576',\n",
    " 'wsj_1579',\n",
    " 'wsj_1608',\n",
    " 'wsj_1625',\n",
    " 'wsj_1638',\n",
    " 'wsj_1653',\n",
    " 'wsj_1716',\n",
    " 'wsj_1771',\n",
    " 'wsj_1785',\n",
    " 'wsj_1801',\n",
    " 'wsj_1805',\n",
    " 'wsj_1807',\n",
    " 'wsj_1834',\n",
    " 'wsj_1836',\n",
    " 'wsj_1838',\n",
    " 'wsj_1845',\n",
    " 'wsj_1933',\n",
    " 'wsj_1942',\n",
    " 'wsj_1954',\n",
    " 'wsj_1955',\n",
    " 'wsj_1956',\n",
    " 'wsj_1958',\n",
    " 'wsj_1979',\n",
    " 'wsj_1993',\n",
    " 'wsj_1995',\n",
    " 'wsj_2035',\n",
    " 'wsj_2050',\n",
    " 'wsj_2090',\n",
    " 'wsj_2097',\n",
    " 'wsj_2117',\n",
    " 'wsj_2122',\n",
    " 'wsj_2134',\n",
    " 'wsj_2137',\n",
    " 'wsj_2147',\n",
    " 'wsj_2170',\n",
    " 'wsj_2171',\n",
    " 'wsj_2173',\n",
    " 'wsj_2174',\n",
    " 'wsj_2175',\n",
    " 'wsj_2176',\n",
    " 'wsj_2177',\n",
    " 'wsj_2178',\n",
    " 'wsj_2179',\n",
    " 'wsj_2180',\n",
    " 'wsj_2181',\n",
    " 'wsj_2182',\n",
    " 'wsj_2183',\n",
    " 'wsj_2184',\n",
    " 'wsj_2185',\n",
    " 'wsj_2186',\n",
    " 'wsj_2187',\n",
    " 'wsj_2188',\n",
    " 'wsj_2189',\n",
    " 'wsj_2190',\n",
    " 'wsj_2191',\n",
    " 'wsj_2192',\n",
    " 'wsj_2193',\n",
    " 'wsj_2194',\n",
    " 'wsj_2195',\n",
    " 'wsj_2196',\n",
    " 'wsj_2197',\n",
    " 'wsj_2198',\n",
    " 'wsj_2199',\n",
    " 'wsj_2220',\n",
    " 'wsj_2236',\n",
    " 'wsj_2263',\n",
    " 'wsj_2279',\n",
    " 'wsj_2283',\n",
    " 'wsj_2284',\n",
    " 'wsj_2285',\n",
    " 'wsj_2286',\n",
    " 'wsj_2287',\n",
    " 'wsj_2288',\n",
    " 'wsj_2289',\n",
    " 'wsj_2290',\n",
    " 'wsj_2291',\n",
    " 'wsj_2292',\n",
    " 'wsj_2293',\n",
    " 'wsj_2294',\n",
    " 'wsj_2295',\n",
    " 'wsj_2296',\n",
    " 'wsj_2297',\n",
    " 'wsj_2298',\n",
    " 'wsj_2299',\n",
    " 'wsj_2304',\n",
    " 'wsj_2307',\n",
    " 'wsj_2312',\n",
    " 'wsj_2318',\n",
    " 'wsj_2349',\n",
    " 'wsj_2385',\n",
    " 'wsj_2388',\n",
    " 'wsj_2389',\n",
    " 'wsj_2390']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {}\n",
    "for f in files:\n",
    "    doc_path = os.path.join(folder_path, f)\n",
    "    doc = get_doc(doc_path)\n",
    "    all_data[f] = {'sentences': doc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pdtb-parse.json', 'w') as f:\n",
    "    json.dump(all_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_path = '/home/pengfei/data/pdtb3-dataset/all/conll/train/pdtb-parses.json'\n",
    "# with open(json_path,  'r') as f:\n",
    "#     parse_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
